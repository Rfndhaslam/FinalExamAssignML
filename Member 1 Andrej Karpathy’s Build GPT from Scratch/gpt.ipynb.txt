{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GPT\n",
    "this is my final exam task. building a GPT tutorial by Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor pustaka requests untuk mengirim permintaan HTTP\n",
    "import requests\n",
    "\n",
    "# URL dari file teks yang akan diunduh\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "# Mengirim permintaan HTTP GET untuk mendapatkan data dari URL\n",
    "response = requests.get(url)\n",
    "# Membuka atau membuat file lokal bernama 'input.txt' dalam mode tulis dengan encoding UTF-8\n",
    "with open(\"input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # Menulis isi teks yang diambil dari URL ke dalam file lokal\n",
    "    f.write(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuka file 'input.txt' dalam mode baca ('r') dengan encoding UTF-8\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    # Membaca seluruh isi file dan menyimpannya dalam variabel 'text'\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text)) # Menampilkan panjang dari var text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan 1000 karakter pertama dari variabel 'text'\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Mendapatkan semua karakter unik yang ada di dalam teks\n",
    "chars = sorted(list(set(text)))\n",
    "# Menghitung jumlah total karakter unik\n",
    "vocab_size = len(chars)\n",
    "# Menampilkan semua karakter unik sebagai satu string\n",
    "print(''.join(chars))\n",
    "# Menampilkan jumlah karakter unik (ukuran kosa kata)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Membuat pemetaan dari karakter ke integer\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# Membuat pemetaan dari integer ke karakter\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# Encoder: fungsi lambda untuk mengubah string menjadi daftar integer berdasarkan pemetaan 'stoi'\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# Decoder: fungsi lambda untuk mengubah daftar integer menjadi string berdasarkan pemetaan 'itos'\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "# Mencetak hasil encoding string \"hii there\" menjadi daftar integer\n",
    "print(encode(\"hii there\"))\n",
    "# Mencetak hasil decoding dari daftar integer kembali ke string asli\n",
    "print(decode(encode(\"hii there\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Mengimpor pustaka PyTorch untuk bekerja dengan tensor\n",
    "import torch  # PyTorch: https://pytorch.org\n",
    "\n",
    "# Mengubah seluruh teks menjadi daftar integer menggunakan fungsi 'encode' dan menyimpannya dalam tensor PyTorch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# Menampilkan ukuran tensor (jumlah elemen) dan tipe datanya\n",
    "print(data.shape, data.dtype)\n",
    "# Menampilkan 1000 elemen pertama dari tensor 'data' yang sebelumnya adalah 1000 karakter pertama teks\n",
    "print(data[:1000])  # Representasi numerik dari teks untuk model seperti GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung indeks pembatas untuk membagi data menjadi set pelatihan dan validasi\n",
    "n = int(0.9 * len(data))  # 90% dari panjang data akan digunakan untuk pelatihan\n",
    "# Membagi data menjadi set pelatihan (90% pertama dari data)\n",
    "train_data = data[:n]\n",
    "# Membagi data menjadi set validasi (10% sisanya dari data)\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mendefinisikan ukuran blok (block_size) untuk model\n",
    "block_size = 8\n",
    "\n",
    "# Mengambil data awal dari train_data sebanyak block_size + 1 elemen\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# Mengambil blok pertama dari train_data untuk input (x)\n",
    "x = train_data[:block_size]\n",
    "\n",
    "# Mengambil blok pertama dari train_data untuk target (y), dimulai dari indeks ke-1\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "# Iterasi melalui setiap indeks dalam blok\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # Mengambil konteks (input) dari awal hingga indeks saat ini (t)\n",
    "    target = y[t] # Menentukan target yang sesuai dari indeks ke-t dalam y    \n",
    "    print(f\"when input is {context} the target: {target}\") # Mencetak pasangan input (context) dan target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Menetapkan seed manual untuk memastikan hasil yang dapat direproduksi\n",
    "batch_size = 4  # Jumlah sequence independen yang akan diproses secara paralel\n",
    "block_size = 8  # Panjang konteks maksimum untuk prediksi\n",
    "\n",
    "def get_batch(split):  # Fungsi untuk menghasilkan batch data\n",
    "    data = train_data if split == 'train' else val_data  # Memilih data pelatihan atau validasi\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # Mengambil indeks acak untuk batch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # Membuat input x dari indeks acak\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # Membuat target y yang bergeser satu langkah\n",
    "    return x, y  # Mengembalikan input dan target\n",
    "\n",
    "xb, yb = get_batch('train')  # Mengambil batch dari data pelatihan\n",
    "print('inputs:')  # Menampilkan input\n",
    "print(xb.shape)  # Bentuk tensor input\n",
    "print(xb)  # Data input\n",
    "print('targets:')  # Menampilkan target\n",
    "print(yb.shape)  # Bentuk tensor target\n",
    "print(yb)  # Data target\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):  # Iterasi melalui dimensi batch\n",
    "    for t in range(block_size):  # Iterasi melalui dimensi waktu\n",
    "        context = xb[b, :t+1]  # Mengambil konteks dari awal hingga indeks saat ini\n",
    "        target = yb[b, t]  # Menentukan target untuk waktu saat ini\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")  # Menampilkan pasangan konteks dan target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # Input dari kita untuk transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch  # Impor PyTorch\n",
    "import torch.nn as nn  # Impor modul neural network PyTorch\n",
    "from torch.nn import functional as F  # Impor fungsi utilitas untuk neural network\n",
    "torch.manual_seed(1337)  # Menetapkan seed manual untuk hasil yang dapat direproduksi\n",
    "\n",
    "class BigramLanguageModel(nn.Module):  # Definisi model bahasa bigram\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()  # Inisialisasi kelas dasar\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # Lookup table untuk memprediksi token berikutnya\n",
    "\n",
    "    def forward(self, idx, targets=None):  # Metode forward untuk input dan (opsional) target\n",
    "        logits = self.token_embedding_table(idx)  # Menghitung logits (B, T, C)\n",
    "\n",
    "        if targets is None:  # Jika tidak ada target, tidak menghitung loss\n",
    "            loss = None\n",
    "        else:  # Jika ada target, hitung cross-entropy loss\n",
    "            B, T, C = logits.shape  # Dekomposisi bentuk tensor\n",
    "            logits = logits.view(B*T, C)  # Merapikan logits ke bentuk (B*T, C)\n",
    "            targets = targets.view(B*T)  # Merapikan target ke bentuk (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)  # Hitung loss dengan cross-entropy\n",
    "\n",
    "        return logits, loss  # Kembalikan logits dan loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):  # Metode untuk menghasilkan token baru\n",
    "        for _ in range(max_new_tokens):  # Ulangi hingga mencapai jumlah token baru\n",
    "            logits, loss = self(idx)  # Hitung logits dari input saat ini\n",
    "            logits = logits[:, -1, :]  # Fokus hanya pada langkah waktu terakhir (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)  # Hitung probabilitas dengan softmax (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sampling token berikutnya (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # Tambahkan token baru ke sequence (B, T+1)\n",
    "        return idx  # Kembalikan sequence yang dihasilkan\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)  # Inisialisasi model dengan ukuran kosa kata\n",
    "logits, loss = m(xb, yb)  # Lakukan forward pass dengan input xb dan target yb\n",
    "print(logits.shape)  # Bentuk tensor logits\n",
    "print(loss)  # Nilai loss\n",
    "\n",
    "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))  # Menghasilkan teks baru dan mendekodenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)  # Membuat optimizer AdamW dengan parameter model dan learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.587916374206543\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Menentukan ukuran batch, yaitu jumlah sequence yang diproses dalam sekali iterasi\n",
    "for steps in range(100):  # Looping sebanyak 100 langkah (dapat ditingkatkan untuk hasil yang lebih baik)\n",
    "\n",
    "    xb, yb = get_batch('train')  # Mengambil batch data pelatihan (input dan target)\n",
    "\n",
    "    logits, loss = m(xb, yb)  # Mengevaluasi loss dengan melakukan forward pass\n",
    "    optimizer.zero_grad(set_to_none=True)  # Mengatur gradien optimizer menjadi nol sebelum backpropagation\n",
    "    loss.backward()  # Melakukan backpropagation untuk menghitung gradien loss terhadap parameter\n",
    "    optimizer.step()  # Memperbarui parameter model menggunakan optimizer\n",
    "\n",
    "print(loss.item())  # Menampilkan nilai loss terakhir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
      "3Q&sGlvHQ?mqSq-eON\n",
      "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
      "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
      "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
      "&WDdP!Ko,px\n",
      "x\n",
      "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
      "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))  # Menghasilkan teks baru dan mencetak hasilnya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # Menetapkan seed untuk hasil acak yang dapat direproduksi\n",
    "a = torch.tril(torch.ones(3, 3))  # Membuat matriks segitiga bawah 3x3 dengan nilai 1\n",
    "a = a / torch.sum(a, 1, keepdim=True)  # Menormalisasi setiap baris matriks sehingga jumlahnya menjadi 1\n",
    "b = torch.randint(0, 10, (3, 2)).float()  # Membuat matriks 3x2 dengan nilai acak dari 0 hingga 9, diubah menjadi float\n",
    "c = a @ b  # Melakukan perkalian matriks antara 'a' dan 'b'\n",
    "print('a=')  # Mencetak matriks 'a'\n",
    "print(a)  # Tampilkan nilai matriks 'a'\n",
    "print('--')\n",
    "print('b=')  # Mencetak matriks 'b'\n",
    "print(b)  # Tampilkan nilai matriks 'b'\n",
    "print('--')\n",
    "print('c=')  # Mencetak hasil perkalian matriks 'c'\n",
    "print(c)  # Tampilkan nilai matriks 'c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Menetapkan seed untuk hasil acak yang dapat direproduksi\n",
    "B, T, C = 4, 8, 2  # Mendefinisikan ukuran: batch (B=4), time steps (T=8), channels/features (C=2)\n",
    "x = torch.randn(B, T, C)  # Membuat tensor acak dengan bentuk (4, 8, 2)\n",
    "x.shape  # Mengembalikan bentuk tensor sebagai output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))  # Membuat tensor nol dengan ukuran (B, T, C) untuk menyimpan hasil\n",
    "for b in range(B):  # Iterasi melalui dimensi batch\n",
    "    for t in range(T):  # Iterasi melalui dimensi waktu\n",
    "        xprev = x[b, :t+1]  # Mengambil semua elemen hingga waktu ke-t dari batch ke-b (berukuran (t+1, C))\n",
    "        xbow[b, t] = torch.mean(xprev, 0)  # Menghitung rata-rata elemen sepanjang waktu sebelumnya (dimensi 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))  # Membuat matriks segitiga bawah ukuran (T, T) dengan nilai 1\n",
    "wei = wei / wei.sum(1, keepdim=True)  # Menormalisasi setiap baris matriks sehingga jumlah elemen di baris tersebut menjadi 1\n",
    "xbow2 = wei @ x  # Melakukan perkalian matriks (T, T) dengan tensor x (B, T, C), hasilnya (B, T, C)\n",
    "torch.allclose(xbow, xbow2)  # Memeriksa apakah `xbow` dan `xbow2` hampir sama (elemen-elemen yang nilainya mendekati sama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))  # Membuat matriks segitiga bawah ukuran (T, T) dengan nilai 1\n",
    "wei = torch.zeros((T, T))  # Membuat matriks nol dengan ukuran (T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # Mengisi elemen di atas diagonal dengan nilai negatif tak hingga (-inf)\n",
    "wei = F.softmax(wei, dim=-1)  # Menggunakan Softmax untuk membuat elemen di setiap baris menjadi probabilitas (jumlah 1)\n",
    "xbow3 = wei @ x  # Melakukan perkalian matriks segitiga bawah berbobot dengan tensor x\n",
    "torch.allclose(xbow, xbow3)  # Memeriksa apakah xbow dan xbow3 memiliki elemen yang hampir sama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Menetapkan seed acak untuk hasil yang dapat direproduksi\n",
    "B, T, C = 4, 8, 32  # Mendefinisikan batch size (B=4), time steps (T=8), dan channels (C=32)\n",
    "x = torch.randn(B, T, C)  # Membuat tensor input acak dengan ukuran (B, T, C)\n",
    "\n",
    "head_size = 16  # Ukuran kepala untuk self-attention\n",
    "key = nn.Linear(C, head_size, bias=False)  # Linear layer untuk menghasilkan key\n",
    "query = nn.Linear(C, head_size, bias=False)  # Linear layer untuk menghasilkan query\n",
    "value = nn.Linear(C, head_size, bias=False)  # Linear layer untuk menghasilkan value\n",
    "\n",
    "k = key(x)  # Menghitung key dari input x, ukuran (B, T, 16)\n",
    "q = query(x)  # Menghitung query dari input x, ukuran (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)  # Menghitung dot product antara query dan key, ukuran (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))  # Membuat matriks segitiga bawah (T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # Masking elemen di atas diagonal dengan nilai -inf\n",
    "wei = F.softmax(wei, dim=-1)  # Normalisasi dengan Softmax untuk membuat distribusi probabilitas (B, T, T)\n",
    "\n",
    "v = value(x)  # Menghitung value dari input x, ukuran (B, T, 16)\n",
    "out = wei @ v  # Menghitung weighted aggregation dari value, ukuran (B, T, 16)\n",
    "\n",
    "out.shape  # Menampilkan bentuk output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]  # Mengakses matriks perhatian (attention matrix) untuk batch pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)  # Membuat tensor key acak berukuran (B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)  # Membuat tensor query acak berukuran (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5  # Menghitung perhatian dengan scaled dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()  # Menghitung variansi elemen-elemen dalam tensor k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()  # Menghitung variansi elemen-elemen dalam tensor q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()  # Menghitung variansi elemen-elemen dalam tensor wei (skor perhatian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)  # Menerapkan fungsi Softmax pada tensor, menghasilkan distribusi probabilitas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)  # Mengalikan input dengan 8 sebelum Softmax, menyebabkan distribusi menjadi lebih tajam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d:  # Implementasi Layer Normalization 1D\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):  # Inisialisasi parameter\n",
    "    self.eps = eps  # Nilai epsilon untuk mencegah pembagian dengan nol\n",
    "    self.gamma = torch.ones(dim)  # Parameter skala (gamma) dengan nilai awal 1\n",
    "    self.beta = torch.zeros(dim)  # Parameter offset (beta) dengan nilai awal 0\n",
    "\n",
    "  def __call__(self, x):  # Memanggil objek sebagai fungsi untuk forward pass\n",
    "    xmean = x.mean(1, keepdim=True)  # Menghitung mean dari setiap vektor input sepanjang dimensi 1\n",
    "    xvar = x.var(1, keepdim=True)  # Menghitung variance dari setiap vektor input sepanjang dimensi 1\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalisasi (mean 0, variance 1)\n",
    "    self.out = self.gamma * xhat + self.beta  # Aplikasi parameter skala dan offset\n",
    "    return self.out  # Mengembalikan output setelah normalisasi\n",
    "\n",
    "  def parameters(self):  # Mengembalikan parameter trainable\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)  # Menetapkan seed acak untuk hasil yang dapat direproduksi\n",
    "module = LayerNorm1d(100)  # Membuat modul LayerNorm1d dengan dimensi 100\n",
    "x = torch.randn(32, 100)  # Membuat tensor input dengan batch size 32 dan dimensi 100\n",
    "x = module(x)  # Melakukan forward pass dengan LayerNorm1d\n",
    "x.shape  # Mengembalikan bentuk output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0].mean(), x[:, 0].std()  # Menghitung mean dan standar deviasi fitur pertama (kolom ke-0) di seluruh batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :].mean(), x[0, :].std()  # Menghitung mean dan standar deviasi semua fitur dari satu sampel dalam batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4195, val loss 2.4335\n",
      "step 400: train loss 2.3503, val loss 2.3567\n",
      "step 500: train loss 2.2965, val loss 2.3130\n",
      "step 600: train loss 2.2410, val loss 2.2497\n",
      "step 700: train loss 2.2055, val loss 2.2191\n",
      "step 800: train loss 2.1631, val loss 2.1859\n",
      "step 900: train loss 2.1247, val loss 2.1508\n",
      "step 1000: train loss 2.1020, val loss 2.1285\n",
      "step 1100: train loss 2.0718, val loss 2.1198\n",
      "step 1200: train loss 2.0384, val loss 2.0802\n",
      "step 1300: train loss 2.0270, val loss 2.0661\n",
      "step 1400: train loss 1.9934, val loss 2.0376\n",
      "step 1500: train loss 1.9701, val loss 2.0303\n",
      "step 1600: train loss 1.9620, val loss 2.0473\n",
      "step 1700: train loss 1.9414, val loss 2.0150\n",
      "step 1800: train loss 1.9072, val loss 1.9971\n",
      "step 1900: train loss 1.9076, val loss 1.9851\n",
      "step 2000: train loss 1.8862, val loss 1.9958\n",
      "step 2100: train loss 1.8741, val loss 1.9773\n",
      "step 2200: train loss 1.8598, val loss 1.9623\n",
      "step 2300: train loss 1.8547, val loss 1.9512\n",
      "step 2400: train loss 1.8422, val loss 1.9412\n",
      "step 2500: train loss 1.8163, val loss 1.9432\n",
      "step 2600: train loss 1.8245, val loss 1.9374\n",
      "step 2700: train loss 1.8112, val loss 1.9344\n",
      "step 2800: train loss 1.8037, val loss 1.9243\n",
      "step 2900: train loss 1.8014, val loss 1.9263\n",
      "step 3000: train loss 1.7958, val loss 1.9197\n",
      "step 3100: train loss 1.7696, val loss 1.9198\n",
      "step 3200: train loss 1.7519, val loss 1.9086\n",
      "step 3300: train loss 1.7574, val loss 1.9071\n",
      "step 3400: train loss 1.7569, val loss 1.8985\n",
      "step 3500: train loss 1.7379, val loss 1.8953\n",
      "step 3600: train loss 1.7263, val loss 1.8869\n",
      "step 3700: train loss 1.7306, val loss 1.8870\n",
      "step 3800: train loss 1.7210, val loss 1.8904\n",
      "step 3900: train loss 1.7230, val loss 1.8729\n",
      "step 4000: train loss 1.7144, val loss 1.8641\n",
      "step 4100: train loss 1.7098, val loss 1.8750\n",
      "step 4200: train loss 1.7062, val loss 1.8662\n",
      "step 4300: train loss 1.6977, val loss 1.8477\n",
      "step 4400: train loss 1.7078, val loss 1.8626\n",
      "step 4500: train loss 1.6871, val loss 1.8438\n",
      "step 4600: train loss 1.6862, val loss 1.8312\n",
      "step 4700: train loss 1.6843, val loss 1.8432\n",
      "step 4800: train loss 1.6662, val loss 1.8433\n",
      "step 4900: train loss 1.6685, val loss 1.8307\n",
      "step 4999: train loss 1.6633, val loss 1.8200\n",
      "\n",
      "ROMEO:\n",
      "But you froth him, what wilth humb.\n",
      "\n",
      "WARTINSA:\n",
      "I life like, to too wherefings,\n",
      "Or weal! savied to thy! but too your not one you gliman;\n",
      "This Iell threws you\n",
      "And than sleen thus mingred, by\n",
      "shat fair comes wowe but and rithre souls shall; there some not.\n",
      "\n",
      "LUCIO:\n",
      "Lord,----\n",
      "But thou sging them this my frecepers?\n",
      "This thou sovore.\n",
      "\n",
      "ISABELLA:\n",
      "The pitsade but grove to him, he's subdot!\n",
      "Thy kneel wimith tybreathing was naje to all,\n",
      "Whelt endeeperly comfsir, whom thou strave so grave.\n",
      "\n",
      "MENCUS Mennerily, tompy begs to die;\n",
      "Haid the stabts his will havolss you,\n",
      "The mustict? the him? Mrancese in some, I and pitaty's!\n",
      "Your slow here him, and that, let that conpert wat you\n",
      "Why deaths!\n",
      "As that eglights in made than one is no drepittion with!\n",
      "He affecry prinritto-forroced doge,\n",
      "With fly more immlaous, I mast,\n",
      "While us fecend, how-fier, I'll that confect to-marrance;\n",
      "Nay the hlanged this armsond fliveriant.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Hew pawe, nave foonum stravoing, not or cheaves? We hy with.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "HENRY Mear to Liesuity no and the men\n",
      "Hus what farther with than war. What bare, I usquelch\n",
      "Frown movedery not duke with you arth,\n",
      "\n",
      "thy forth have he bands wondy worse is senst my long.\n",
      "\n",
      "QUEN MARGARETLET:\n",
      "Yes prunt, bost stoly casmer bown!\n",
      "'Dau I sot read thou to? sooff in thus me\n",
      "Lord that too the his a king thing,\n",
      "By beny the said home, for we sull his name.\n",
      "\n",
      "NORTHUMBRARD:\n",
      "\n",
      "HENTI:\n",
      "O, the his it no are me, or your whom and by\n",
      "Wonteety, night will, we e slorm:\n",
      "That gettrew and to that his say\n",
      "as to be, prower, so good their citner,\n",
      "But unter, tonding hont's freen made;\n",
      "It renever. Kpee nonett, thou make-banited?\n",
      "\n",
      "LORCIUS:\n",
      "Or as Juriole true to at mover a desry, I whilse 'happed.\n",
      "\n",
      "CORIOLE:\n",
      "\n",
      "First both,\n",
      "Where, whose to the bote thmy bruut I ditsp.\n",
      "\n",
      "MENRY VOLINGBRUS:\n",
      "Genter's, by? Richardowing you wind you,\n",
      "Non't, more is in meethet, thou prost to in was thus;\n",
      "For toward merring that waste; Dirdst weye tindery what it to repon hims;\n",
      "And the 'tis lacect if Con to suritignned.\n",
      "\n",
      "ARTIO\n"
     ]
    }
   ],
   "source": [
    "import torch  # Import PyTorch library\n",
    "import torch.nn as nn  # Import neural network module\n",
    "from torch.nn import functional as F  # Import functional interface\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # Jumlah urutan independen yang diproses secara paralel\n",
    "block_size = 32  # Panjang konteks maksimum untuk prediksi\n",
    "max_iters = 5000  # Jumlah iterasi pelatihan\n",
    "eval_interval = 100  # Interval evaluasi\n",
    "learning_rate = 1e-3  # Laju pembelajaran\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Gunakan GPU jika tersedia\n",
    "eval_iters = 200  # Jumlah iterasi untuk evaluasi\n",
    "n_embd = 64  # Dimensi embedding\n",
    "n_head = 4  # Jumlah head dalam multi-head attention\n",
    "n_layer = 4  # Jumlah layer transformer\n",
    "dropout = 0.0  # Tingkat dropout\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)  # Set seed untuk reproduktifitas\n",
    "\n",
    "# Membaca data teks\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Mengidentifikasi karakter unik\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# Membuat mapping karakter ke integer dan sebaliknya\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # Encoder: string ke list integer\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # Decoder: list integer ke string\n",
    "\n",
    "# Split data menjadi train dan validation\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))  # 90% data untuk pelatihan\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Fungsi untuk mengambil batch data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" Satu head dari self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # Layer key\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Layer query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  # Layer value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # Masking lower triangular\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Menghitung skor perhatian\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5  # Skor perhatian\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Masking\n",
    "        wei = F.softmax(wei, dim=-1)  # Normalisasi\n",
    "        wei = self.dropout(wei)  # Terapkan dropout\n",
    "        # Agregasi nilai berdasarkan perhatian\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-head self-attention secara paralel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # Daftar head\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # Proyeksi linear\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # Gabungkan output semua head\n",
    "        out = self.dropout(self.proj(out))  # Proyeksi dan dropout\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" Layer feed-forward sederhana \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Layer linear pertama\n",
    "            nn.ReLU(),  # Aktivasi non-linear\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Layer linear kedua\n",
    "            nn.Dropout(dropout),  # Dropout layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Blok Transformer: komunikasi diikuti oleh komputasi \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # Ukuran setiap head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Self-attention\n",
    "        self.ffwd = FeedFoward(n_embd)  # Feed-forward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd)  # Layer normalization pertama\n",
    "        self.ln2 = nn.LayerNorm(n_embd)  # Layer normalization kedua\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))  # Residual connection dengan self-attention\n",
    "        x = x + self.ffwd(self.ln2(x))  # Residual connection dengan feed-forward\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\" Model Bahasa Bigram yang sederhana \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # Embedding token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # Embedding posisi\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])  # Transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # Layer normalization akhir\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # Head untuk prediksi token\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # Embedding token\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # Embedding posisi\n",
    "        x = tok_emb + pos_emb  # Gabungkan embedding token dan posisi\n",
    "        x = self.blocks(x)  # Pass melalui transformer blocks\n",
    "        x = self.ln_f(x)  # Layer normalization akhir\n",
    "        logits = self.lm_head(x)  # Prediksi logits\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)  # Hitung loss\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  # Crop konteks\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # Fokus pada timestep terakhir\n",
    "            probs = F.softmax(logits, dim=-1)  # Probabilitas token berikutnya\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sampling token\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # Tambahkan token ke sequence\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()  # Inisialisasi model\n",
    "m = model.to(device)  # Pindahkan model ke device\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')  # Cetak jumlah parameter\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # Inisialisasi optimizer\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")  # Cetak loss\n",
    "\n",
    "    xb, yb = get_batch('train')  # Ambil batch data\n",
    "\n",
    "    logits, loss = model(xb, yb)  # Hitung loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Reset gradien\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update parameter\n",
    "\n",
    "# Generate teks dari model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Inisialisasi konteks\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))  # Cetak teks yang dihasilkan\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
