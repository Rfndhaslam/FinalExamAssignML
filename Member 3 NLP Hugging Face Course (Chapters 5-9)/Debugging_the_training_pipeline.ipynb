{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrDz00d-atv2"
      },
      "source": [
        "# Debugging the training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WozLWpgYatv4"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b58dhRVXatv4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypuRSnMnatv5",
        "outputId": "10eb7a01-8987-4b3f-9a3a-549dd26f5f20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: You have to specify either input_ids or inputs_embeds'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=raw_datasets[\"train\"],\n",
        "    eval_dataset=raw_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XCdZPmqatv6",
        "outputId": "3d19d5be-6223-413d-d2ec-f86ea128e2ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
              " 'idx': 0,\n",
              " 'label': 1,\n",
              " 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_UF8_WWatv6",
        "outputId": "dbb32a8a-9674-49a4-c882-ba6a7326fd6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: expected sequence of length 43 at dim 1 (got 37)'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKZuFO2gatv6",
        "outputId": "d137485d-6043-461e-a398-8f1be9c8eac9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9rAEpNjatv7",
        "outputId": "2aba3c58-2d90-4e2d-cba6-6f592f2ebf15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI__wTgxatv7",
        "outputId": "5b489b79-e521-4dda-b13a-57c35bfa76c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DjV9EVKatv7",
        "outputId": "e7b26b73-dc64-4120-c616-b9d16227d3c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3dfj6FLatv7",
        "outputId": "826d5fff-4fb9-4046-dc94-ec3c36ba7b61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainer.train_dataset[0][\"attention_mask\"]) == len(\n",
        "    trainer.train_dataset[0][\"input_ids\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV6LIx-Katv8",
        "outputId": "f451209d-0e3a-447b-8078-0661f27b8672"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C_8rTG3atv8",
        "outputId": "4748d3d4-75d0-45ed-8143-f7aad29db13e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['entailment', 'neutral', 'contradiction']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset.features[\"label\"].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQe46FPXatv8",
        "outputId": "67d8a5a0-2125-4f07-9fa2-9bf7249edcac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)\n",
              "    105                 batch[k] = torch.stack([f[k] for f in features])\n",
              "    106             else:\n",
              "--> 107                 batch[k] = torch.tensor([f[k] for f in features])\n",
              "    108 \n",
              "    109     return batch\n",
              "\n",
              "ValueError: expected sequence of length 45 at dim 1 (got 76)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jqY-Tiratv8",
        "outputId": "2b675295-9716-4fb4-bc76-d175bcaa7c6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umLPRjKZatv8",
        "outputId": "d372f733-0f6f-450f-b3e0-59a22cb326aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHPTyO-6atv8"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "batch = data_collator([trainer.train_dataset[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzrIIiPfatv8"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)\n",
        "batch = data_collator([actual_train_set[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwllXw9iatv9"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ctW2rzIatv9",
        "outputId": "caf523d1-0171-4c40-e016-556869372b97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n",
              "   2386         )\n",
              "   2387     if dim == 2:\n",
              "-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "   2389     elif dim == 4:\n",
              "   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "\n",
              "IndexError: Target 2 is out of bounds."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm_KIRKRatv9",
        "outputId": "d669d2fa-60b7-46ac-ff5c-281325a20409"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.config.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aUQJ88Gatv9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L65lAG6qatv9"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbDjVn8Latv9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "outputs = trainer.model.to(device)(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPM9sX8qatv9"
      },
      "outputs": [],
      "source": [
        "loss = outputs.loss\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XczrfWwnatv9"
      },
      "outputs": [],
      "source": [
        "trainer.create_optimizer()\n",
        "trainer.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQLaZZEIatv9",
        "outputId": "377299a9-5602-475d-8974-ee6ca34ad3bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This will take a long time and error out, so you shouldn't run this cell\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e25Tw-Zatv9",
        "outputId": "458b6699-3860-413d-93a2-865a76f1c603"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_BAVqxqatv9"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5pbdlfsatv-",
        "outputId": "45fb60da-7ead-42b3-9ce7-1572e087f990"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = outputs.logits.cpu().numpy()\n",
        "labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfWUjuvbatv-",
        "outputId": "9467aa50-a4f9-4b0c-a41a-19ce251f17b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8, 3), (8,))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib59LK0matv-",
        "outputId": "a5b58c7d-a25a-4303-a9ed-dd660aeb8821"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.625}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9mlyakFatv-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEQ6t58datv-"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "trainer.create_optimizer()\n",
        "\n",
        "for _ in range(20):\n",
        "    outputs = trainer.model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    trainer.optimizer.step()\n",
        "    trainer.optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcsNZcllatv-",
        "outputId": "2c0d4ca0-13bf-48f2-f331-1a1a331b9f99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 1.0}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)\n",
        "preds = outputs.logits\n",
        "labels = batch[\"labels\"]\n",
        "\n",
        "compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Debugging the training pipeline",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}